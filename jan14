test_cost_file_service.py:3: in <module>
    from app import app
..\..\app.py:13: in <module>
    from api.experiment_resource import ExperimentResource, ExperimentsResource, RefreshExperimentsResource
..\..\api\experiment_resource.py:5: in <module>
    from services import experiment_service, notification_service
..\..\services\experiment_service.py:9: in <module>
    from models.experiment_master import ExperimentMaster, experiments_schema, experiment_schema    
..\..\models\__init__.py:1: in <module>
    from .access_groups import AccessGroups
..\..\models\access_groups.py:4: in <module>
    from .base import Base
..\..\models\base.py:9: in <module>
    engine = get_db_engine()
..\..\configs.py:137: in get_db_engine
    return get_ltvo_utils_prod_dev_db_engine()
..\..\configs.py:211: in get_ltvo_utils_prod_dev_db_engine
    client = secretmanager.SecretManagerServiceClient()
C:\Users\10722751\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\cloud\secretmanager_v1\services\secret_manager_service\client.py:481: in __init__
    self._transport = Transport(
C:\Users\10722751\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\cloud\secretmanager_v1\services\secret_manager_service\transports\grpc.py:157: in __init__
    super().__init__(
C:\Users\10722751\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\cloud\secretmanager_v1\services\secret_manager_service\transports\base.py:100: in __init__
    credentials, _ = google.auth.default(
C:\Users\10722751\AppData\Local\Programs\Python\Python311\Lib\site-packages\google\auth\_default.py:691: in default
    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)
E   google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.




import os
import shutil

import sqlalchemy
from google.cloud import secretmanager
from sqlalchemy.orm import sessionmaker

import my_logger
app_logger=my_logger.configure_logger()
ACTIVE_PROFILE = 'LTVO_UTILS_DEV'
app_logger.info(f"Active_Profile{ACTIVE_PROFILE}")
# ENV variable is set in the kubernetes deployment through the GitHub Action deploy step
if os.getenv('ACTIVE_PROFILE'):
    ACTIVE_PROFILE = os.getenv('ACTIVE_PROFILE')


def get_app_configs():
    if ACTIVE_PROFILE == 'LOCAL' or ACTIVE_PROFILE == 'DEV' or ACTIVE_PROFILE == 'STG' or ACTIVE_PROFILE == 'PROD':
        app_logger.info(f"Get_Local_App_configs:{experiment_configs_old}")
        return experiment_configs_old
    if ACTIVE_PROFILE == 'LTVO_UTILS_DEV':
        app_logger.info(f"Get_Dev_App_configs:{experiment_configs_dev}")
        return experiment_configs_dev
    if ACTIVE_PROFILE == 'LTVO_UTILS_STG':
        app_logger.info(f"Get_stage_app_configs:{experiment_configs_stg}")
        return experiment_configs_stg
    if ACTIVE_PROFILE == 'LTVO_UTILS_PROD':
        app_logger.info(f"Get_prod_app_configs:{experiment_configs_prod}")
        return experiment_configs_prod

experiment_configs_old = {
    "KUBEFLOW_CONFIG_JSON" :  "nafc_ui_test.json",
    "KUBEFLOW_PROJECT_ID":"ltvo-ui-fc-dev-3eab",
    "OUTPUT_GCS_BUCKET" :"ltvo-ui-fc-dev-3eab-ltvo-us-a98mqx6h",
    "OUTPUT_GCS_BUCKET_CA" :"ltvo-ui-fc-dev-3eab-ltvo-us-a98mqx6h",
    "ESM_GCS_BUCKET" : "ltvo-ui-fc-dev-3eab-ltvo-us-a98mqx6h",
    "INPUT_GCS_BUCKET": "ltvo-ui-fc-dev-3eab-ltvo-us-a98mqx6h",
    "LTVO_PROJECT_ID":"ltvo-ui-fc-dev-3eab",
    "DSWB_PRJECT_ID":"ltvo-ui-fc-dev-3eab",
    "KUBEFLOW_PROJECT_ID":"ltvo-ui-fc-dev-3eab",
    "DATASET_ID":  "s_ltvo_us"
}

experiment_configs_dev = {
    "KUBEFLOW_CONFIG_JSON" :  "nafc_ui_utils_develop.json",
    "KUBEFLOW_PROJECT_ID":"ltvo-dswb-prod-c423",
    "OUTPUT_GCS_BUCKET" :"ltvo-dswb-prod-c423-ltvo-us-kubeflow-output-eyyroxpx",
    "OUTPUT_GCS_BUCKET_CA" :"ltvo-dswb-prod-c423-ltvo-ca-kubeflow-output-upkno8gr",
    "ESM_GCS_BUCKET" : "ltvo-cost-prod-e8fb-cost-ui-na-h5odhwzl",
    "ESM_GCS_BUCKET_CA": "ltvo-cost-prod-e8fb-cost-ui-na-h5odhwzl",
    "INPUT_GCS_BUCKET": "ltvo-cost-prod-e8fb-cost-ui-na-h5odhwzl",
    "LTVO_PROJECT_ID":"ltvo-utils-prod-eaa5",
    "INIT_PARAMS_PRJ":"ltvo-datalake-prod-dfd5",
    "DSWB_PRJECT_ID":"ltvo-dswb-prod-c423",
    "DATASET_ID":  "s_ltvo_us",
    "ns":"dev",
    "AIE_API_BASE_URL": "http://ltvo-ds.aif-kfp-api-dev:8182",
    "LTVO_IMAGE": "gcr.io/ltvo-dswb-prod-c423/ltvo_kfp_global_cpu_base_runner@sha256:308602797364e43b40a47279cc2893a1b7645098180bc4086dd1545a2b916773",
    "AAG_USER_SERVICE_URL": "https://aag-msgraph-service-2255rb7oba-uk.a.run.app"
}
experiment_configs_stg = {
    "KUBEFLOW_CONFIG_JSON" :  "nafc_ui_utils_stg.json",
    "KUBEFLOW_PROJECT_ID":"ltvo-kubeflow-stg-dc3d",
    "OUTPUT_GCS_BUCKET" :"ltvo-kubeflow-stg-dc3d-ltvo-us-kubeflow-output-pp8batf3",
    "OUTPUT_GCS_BUCKET_CA" :"ltvo-kubeflow-stg-dc3d-ltvo-ca-kubeflow-output-7oeakr3i",
    "ESM_GCS_BUCKET" : "ltvo-cost-prod-e8fb-cost-ui-na-h5odhwzl",
    "ESM_GCS_BUCKET_CA" : "ltvo-cost-prod-e8fb-cost-ui-na-h5odhwzl",
    "INPUT_GCS_BUCKET": "ltvo-cost-prod-e8fb-cost-ui-na-h5odhwzl",
    "LTVO_PROJECT_ID":"ltvo-utils-prod-eaa5",
    "DSWB_PRJECT_ID":"ltvo-kubeflow-stg-dc3d",
    "INIT_PARAMS_PRJ":"ltvo-datalake-prod-dfd5",
    "DATASET_ID":  "s_ltvo_us",
    "ns":"stg",
    "AIE_API_BASE_URL": "http://ltvo-ds.aif-kfp-api-stg:8182",
    "LTVO_IMAGE": 
"gcr.io/ltvo-kubeflow-stg-dc3d/ltvo_kfp_global_cpu_base_runner:v2.21.0-ds",
    "AAG_USER_SERVICE_URL": "https://aag-msgraph-service-2255rb7oba-uk.a.run.app"
}

experiment_configs_prod = {
    "KUBEFLOW_CONFIG_JSON" :  "nafc_ui_utils_prod.json",
    "KUBEFLOW_PROJECT_ID":"ltvo-kubeflow-prod-9306",
    "OUTPUT_GCS_BUCKET" :"ltvo-kubeflow-prod-9306-ltvo-us-kubeflow-output-dx2q4tbu",
    "OUTPUT_GCS_BUCKET_CA" :"ltvo-kubeflow-prod-9306-ltvo-ca-kubeflow-output-z8zwpbu7",
    # "ESM_GCS_BUCKET" : "ltvo-kubeflow-prod-9306-ltvo-us-n43dfp3s",
    "ESM_GCS_BUCKET" : "ltvo-cost-prod-e8fb-cost-ui-na-h5odhwzl",
    "ESM_GCS_BUCKET_CA" : "ltvo-cost-prod-e8fb-cost-ui-na-h5odhwzl",
    "INPUT_GCS_BUCKET": "ltvo-kubeflow-prod-9306-ltvo-us-n43dfp3s",
    "LTVO_PROJECT_ID":"ltvo-utils-prod-eaa5",
    "DSWB_PRJECT_ID":"ltvo-kubeflow-prod-9306",
    "INIT_PARAMS_PRJ":"ltvo-datalake-prod-dfd5",
    "DATASET_ID":  "s_ltvo_us",
    "ns":"prod",
    "AIE_API_BASE_URL": "http://ltvo-ds.aif-kfp-api-prod:8182",
    "LTVO_IMAGE": 
"gcr.io/ltvo-kubeflow-prod-9306/ltvo_kfp_global_cpu_base_runner:v2.21.0-ds",
    "AAG_USER_SERVICE_URL": "https://aag-msgraph-service-2255rb7oba-uk.a.run.app"
}

db_config = {
    "pool_size": 3,
    "max_overflow": 2,
    "pool_timeout": 30,  # 30 seconds
    # [END cloud_sql_mysql_sqlalchemy_timeout]

    # [START cloud_sql_mysql_sqlalchemy_lifetime]
    # 'pool_recycle' is the maximum number of seconds a connection can persist.
    # Connections that live longer than the specified amount of time will be
    # reestablished
    "pool_recycle": 3600,  # 30 minutes
    # [END cloud_sql_mysql_sqlalchemy_lifetime]
}

def get_active_schema():
    active_schema = 'ltvo_dev_schema'
    if ACTIVE_PROFILE == 'STG'  or ACTIVE_PROFILE == 'LTVO_UTILS_STG':
        active_schema = 'ltvo_stg_schema'
    elif ACTIVE_PROFILE == 'PROD' or ACTIVE_PROFILE == 'LTVO_UTILS_PROD':
        active_schema= 'ltvo_prd_schema'
    elif ACTIVE_PROFILE == 'LTVO_UTILS_DEV':
        active_schema = 'ltvo_dev_schema'
    elif ACTIVE_PROFILE == 'LOCAL':
        active_schema = 'public'
    return active_schema

def get_db_engine():
    print("ACTIVE_PROFILE", ACTIVE_PROFILE)
    if ACTIVE_PROFILE == 'LOCAL':
        return get_local_db_engine()
    elif ACTIVE_PROFILE == 'DEV':
        return get_gcp_dev_db_engine()
    elif ACTIVE_PROFILE == 'STG':
        return get_gcp_dev_db_engine()
    elif ACTIVE_PROFILE == 'PROD':
        return get_gcp_dev_db_engine()
    elif ACTIVE_PROFILE == 'LTVO_UTILS_DEV' or ACTIVE_PROFILE == 'LTVO_UTILS_STG' or ACTIVE_PROFILE == 'LTVO_UTILS_PROD':
        return get_ltvo_utils_prod_dev_db_engine()
    else:
        return 'invalid Env'


class SingletonSession:
    _instance = None

    @staticmethod
    def get_instance():
        if SingletonSession._instance is None:
            SingletonSession()
        return SingletonSession._instance

    def __init__(self):
        if SingletonSession._instance is not None:
            raise Exception("SingletonSession is already instantiated")
        else:
            app_logger.info("Attempting to extablish DB connection")
            app_logger.info(f'Attempting to set schema to {get_active_schema()}')
            Session = sessionmaker(bind=get_db_engine())
            self.session = Session()
            self.session.execute(f"SET search_path TO {get_active_schema()}")
            SingletonSession._instance = self


def _init_session():
    app_logger.info("Init Session calledB")
    for tries in range(5):
        try:
            session = _try_connection()
            return session
        except Exception as e:
            app_logger.error(f"DB connection failed {tries} times: {str(e)}")
            continue
    raise Exception("Cannot connect to Databse in __init_session(). Session cannot be created!")



def _try_connection():
    app_logger.info("Returning Singleton Session")
    session = SingletonSession.get_instance().session
    app_logger.info("Session returned")
    app_logger.info(session)
    return session


def get_gcp_dev_db_engine():
    host = "10.132.5.9"  # TODO add secrets
    user = "ltvo-dev"
    passwd = "postgres_ltvo_26"
    print(os.environ['HOME'])
    db = "ltvo_ui_db"
    project_id = 'ltvo-ui-fc-dev-3eab'
    db_string = f"postgresql://{user}:{passwd}@{host}/{db}?sslmode=require" \
                f"&sslrootcert={access_secret(project_id,'SERVER_CERT','/usr/local/share/ca-certificates/server-ca-psg.pem')}" \
                f"&sslcert={access_secret(project_id,'CLIENT_CERT','/usr/local/share/ca-certificates/client-cert-psg.pem')}" \
                f"&sslkey={access_secret(project_id,'CLIENT_KEY','/usr/local/share/ca-certificates/client-key-psg.pem')}"
    os.system("chmod -R 0600 /usr/local/share/ca-certificates")
    engine = sqlalchemy.create_engine(
        # Equivalent URL:
        db_string,
        echo=True
    )
    return engine

def get_ltvo_utils_prod_dev_db_engine():

    def write_string_to_pem(string, pem_file_path):
        with open(pem_file_path, 'w') as pem_file:
            pem_file.write(string)

    app_logger.info('Getting connection string')
    host = "10.123.10.13" # TODO add secrets
    client = secretmanager.SecretManagerServiceClient()
    app_logger.info('Succesfully connected to secret manager')
    project_id = 'ltvo-utils-prod-eaa5'

    # Get the DB certs from secret manager
    server_cert_name = f"projects/{project_id}/secrets/postgresql-ui-fc-server-ca/versions/latest"
    client_cert_name = f"projects/{project_id}/secrets/postgresql-ui-fc-client-cert/versions/latest"
    client_cert_key = f"projects/{project_id}/secrets/postgresql-ui-fc-client-key/versions/latest"
    server_cert_path = "/usr/local/share/ca-certificates/server-ca-psg.pem"
    client_cert_path = "/usr/local/share/ca-certificates/client-cert-psg.pem"
    client_key_path = "/usr/local/share/ca-certificates/client-key-psg.pem"
    os.system("chmod -R 0600 /etc/secret")
    os.system("chmod -R 0600 /usr/local/share/ca-certificates")
    
    try:
        response = client.access_secret_version(name=server_cert_name)
        payload = response.payload.data.decode("UTF-8")
        server_cert = payload.replace("\\n","\n")
        write_string_to_pem(server_cert, server_cert_path)
        app_logger.info(f"Server Cert {server_cert}")
        app_logger.info("Retrieved Server Cert")
    except Exception as e:
        app_logger.error(f"Could not get Server Cert: {str(e)}")
    
    try:
        response = client.access_secret_version(name=client_cert_name)
        payload = response.payload.data.decode("UTF-8")
        write_string_to_pem(payload.replace("\\n","\n"), client_cert_path)
        app_logger.info("Retrieved Client Cert")
    except Exception as e:
        app_logger.error(f"Could not get Client Cert: {str(e)}")
    
    try:
        response = client.access_secret_version(name=client_cert_key)
        payload = response.payload.data.decode("UTF-8")
        write_string_to_pem(payload.replace("\\n","\n"), client_key_path)
        app_logger.info("Retrieved Client Key")
    except Exception as e:
        app_logger.error(f"Could not get Client Key: {str(e)}")

    user = ''
    passwd = ''
    db = ''

    try:
        name = f"projects/{project_id}/secrets/ui-fc-sql/versions/latest"
        response = client.access_secret_version(name=name)
        payload = response.payload.data.decode("UTF-8")
        user = payload.split(',')[0]
        passwd = payload.split(',')[1]
        db = payload.split(',')[2]
        app_logger.info("Retrieved DB Credentials")
    except Exception as e:
        app_logger.error(f"Could not get DB Credentials: {str(e)}")

    # engine = sqlalchemy.create_engine(
    #     sqlalchemy.engine.url.URL.create(
    #         drivername='postgresql+pg8000',
    #         username=user,
    #         password=passwd,
    #         host=host,
    #         port=3306,
    #         database=db,
    #     )
    # )

    os.system("chmod -R 0600 /usr/local/share/ca-certificates")
    server_cert_size = os.stat(server_cert_path).st_size
    client_cert_size = os.stat(client_cert_path).st_size
    client_key_size = os.stat(client_key_path).st_size
    app_logger.info(f"Server cert size: {str(server_cert_size)}")
    app_logger.info(f"Client cert size: {str(client_cert_size)}")
    app_logger.info(f"Client key size: {str(client_key_size)}")


    db_string = f"postgresql://{user}:{passwd}@{host}/{db}?sslmode=verify-ca" \
                f"&sslrootcert={server_cert_path}" \
                f"&sslcert={client_cert_path}" \
                f"&sslkey={client_key_path}"
    engine = sqlalchemy.create_engine(
        # Equivalent URL:
        db_string,
        echo=True
    )
    app_logger.info("BD Connection established")
    return engine


def get_local_db_engine():
    host = os.environ.get('PG_HOST', "psql-db:5432")
    user = os.environ.get('PG_USER', "ltvo_ui_dev")
    passwd = os.environ.get('PG_PASSWORD', "password123")
    db_string = f"postgresql://{user}:{passwd}@{host}/ltvo_ui_dev"
    app_logger.info(f"get_local_db_engine:{db_string}")
    print(db_string)
    engine = sqlalchemy.create_engine(
        # Equivalent URL:
        db_string,
        echo=True
    )
    return engine

def access_secret(project_id, secret_id, file_path, secret_version="latest"):
    sm_client = secretmanager.SecretManagerServiceClient()
    name = f"projects/{project_id}/secrets/{secret_id}/versions/{secret_version}"
    response = sm_client.access_secret_version(request={"name": name})
    secret = response.payload.data.decode("UTF-8")
    f = open(file_path, "w")
    f.write(secret)
    f.close()
    return file_path

host = "localhost"
user = "ltvo_ui_dev"
passwd = "password123"

# host="10.132.5.9"
# user="ltvo-dev"
# passwd="postgres_ltvo_26"
# print(os.environ['HOME'])
# db = "ltvo_ui_dev"


# db_string_gcp = f"postgresql://{user}:{passwd}@{host}/ltvo_ui_db?sslmode=require" \
#             f"&sslrootcert={os.environ['SERVER_CERT']}" \
#             f"&sslcert={os.environ['CLIENT_CERT']}" \
#             f"&sslkey={os.environ['CLIENT_KEY']}"

db_string = f"postgresql://{user}:{passwd}@{host}/ltvo_ui_dev"

# TODO Confirm if these are needed?
supplier_map = {
    "ABC": "Disney",
    "ADSM": "Warner Media",
    "AEN": "AETN",
    "AMC": "AMC",
    "AMTV": "CBSViacom",
    "APL": "Discovery",
    "AZA": "HC2 Broadcasting Holdings Inc",
    "BBCA": "AMC",
    "BEIE": "HC2 Broadcasting Holdings Inc",
    "BET": "CBSViacom",
    "BHER": "CBSViacom",
    "BOOM": "Warner Media",
    "BRVO": "NBC Universal",
    "CBS": "CBSViacom",
    "CC": "Discovery",
    "CCP": "CCPTV",
    "CFE": "ES",
    "CMDY": "CBSViacom",
    "CMT": "CBSViacom",
    "CNBC": "NBC Universal",
    "CNN": "Warner Media",
    "CNNE": "Warner Media",
    "COZ": "NBC Universal",
    "CTD": "CBSViacom",
    "CW": "CW",
    "DAD": "Disney",
    "DAL": "Disney",
    "DAM": "Discovery",
    "DFAM": "Discovery",
    "DFC": "Discovery",
    "DISC": "Discovery",
    "DIY": "Discovery",
    "DLIF": "Discovery",
    "DSE": "Discovery",
    "ENT": "NBC Universal",
    "ESPD": "FOX",
    "ESPN": "Disney",
    "ESPN2": "Disney",
    "ESPNU": "Disney",
    "FOOD": "Discovery",
    "FOX": "FOX",
    "FOXNC": "FOX",
    "FRFM": "Disney",
    "FS1": "FOX",
    "FX": "Disney",
    "FXX": "Disney",
    "FYI": "AETN",
    "GAC": "Discovery",
    "GALA": "Univision",
    "GOLF": "NBC Universal",
    "HALL": "Hallmark",
    "HGTV": "Discovery",
    "HIST": "AETN",
    "HLN": "Warner Media",
    "HMM": "Hallmark",
    "ID": "Discovery",
    "IFC": "AMC",
    "ITN": "ITN",
    "LIF": "AETN",
    "LMN": "AETN",
    "LOGO": "CBSViacom",
    "MSNBC": "NBC Universal",
    "MTV": "CBSViacom",
    "MTV2": "CBSViacom",
    "NAN": "CBSViacom",
    "NBC": "NBC Universal",
    "NBCSN": "NBC Universal",
    "NBU": "NBC Universal",
    "NGC": "Disney",
    "NGWD": "Disney",
    "NICK": "CBSViacom",
    "NKJR": "CBSViacom",
    "NWSY": "Newsy",
    "OWN": "Discovery",
    "OXYG": "NBC Universal",
    "PAR": "CBSViacom",
    "POP": "CBSViacom",
    "PPI": "Sony",
    "SCI": "Discovery",
    "SMTH": "CBSViacom",
    "SPT": "Sony",
    "SUND": "AMC",
    "SYFY": "NBC Universal",
    "TBSC": "Warner Media",
    "TEL": "NBC Universal",
    "TLC": "Discovery",
    "TNT": "Warner Media",
    "TOON": "Warner Media",
    "TRAV": "Discovery",
    "TRI": "Trifecta",
    "TRU": "Warner Media",
    "TUDN": "Univision",
    "TVL": "CBSViacom",
    "TWC": "ES",
    "UKID": "NBC Universal",
    "UMA": "Univision",
    "UNI": "Univision",
    "UNVSO": "NBC Universal",
    "USA": "NBC Universal",
    "VH1": "CBSViacom",
    "VICE": "AETN",
    "WB": "Warner Media",
    "WETV": "AMC",
    "WGNA": "WGN"}









@patch("your_module.get_app_configs")
    @patch("your_module.app_logger")
    @patch("your_module.storage.bucket.Bucket")
    @patch("your_module.storage.blob.Blob")
    @patch("your_module.secretmanager.SecretManagerServiceClient")
    def test_get_csv_file_names_mocked_function(self, mock_secret_manager, mock_blob, mock_bucket, mock_logger, mock_get_app_configs):
        # Mocking necessary objects
        mock_blobs = [Mock(name="file1.csv"), Mock(name="file2.csv")]

        mock_get_app_configs.return_value = {
            "ESM_GCS_BUCKET": "your_bucket",
            "NAMESPACE": "your_namespace"
        }

        # Mocking the SecretManagerServiceClient
        mock_secret_manager_instance = mock_secret_manager.return_value
        mock_secret_manager_instance.access_secret_version.return_value.payload.data.decode.return_value = "mocked_secret_value"

        # Mocking relevant methods in the module
        mock_bucket_instance = mock_bucket.return_value
        mock_bucket_instance.list_blobs.return_value = mock_blobs

        # Call the function with test data
        result = get_csv_file_names("US", "category")

        # Assertions
        mock_logger.info.assert_called_with("Bucket_name is your_bucket")
        mock_logger.info.assert_called_with("Namespace:your_namespace")
        mock_logger.info.assert_called_with("file path in gcs Bucket:ltvo_your_namespace/na/US/category")
        mock_logger.info.assert_called_with('final_files info:[{"csv_file_name": "file1.csv"}, {"csv_file_name": "file2.csv"}]')

        expected_result = [{"csv_file_name": "file1.csv"}, {"csv_file_name": "file2.csv"}]
        self.assertEqual(result, expected_result)








class BigQueryResults:
    def __init__(self, lst):
        self.lst = lst

    def result(self):
        return self.lst

    @property
    def total_rows(self):
        return len(self.lst)


class BigQueryInteractiveCountsRow:
    def __init__(self, segment_id, count):
        self.segment_id = segment_id
        self.ct = count


class RequestsResponse:
    def __init__(self, response_data: Any, http_status: int) -> None:
        self._response_data = response_data
        self._http_status = http_status

    @property
    def ok(self):
        return self._http_status == 200

    @property
    def data(self) -> Any:
        return self._response_data

    @property
    def text(self) -> str:
        return json.dumps(self._response_data, sort_keys=True)

    def json(self) -> str:
        return self._response_data

    @property
    def status_code(self) -> int:
        return self._http_status







class MockStorageClient:
    def __init__(self, mock_blobs):
        self.mock_blobs = mock_blobs

    class MockBlob:
        def __init__(self, name):
            self.name = name

    def bucket(self, bucket_name):
        return MockStorageClient.MockBucket(self.mock_blobs)

    class MockBucket:
        def __init__(self, mock_blobs):
            self.mock_blobs = mock_blobs

        def list_blobs(self, prefix=None):
            return [MockStorageClient.MockBlob(name) for name in self.mock_blobs]

# Use this class to mock the Cloud Storage client in your test
@patch("your_module.storage.Client", MockStorageClient)
def test_get_csv_file_names_mocked_function(self, mock_secret_manager, mock_blob, mock_bucket, mock_logger, mock_get_app_configs):
    # Mocking necessary objects
    mock_blobs = ["file1.csv", "file2.csv"]

    mock_get_app_configs.return_value = {
        "ESM_GCS_BUCKET": "your_bucket",
        "NAMESPACE": "your_namespace"
    }

    # Mocking the SecretManagerServiceClient
    mock_secret_manager_instance = mock_secret_manager.return_value
    mock_secret_manager_instance.access_secret_version.return_value.payload.data.decode.return_value = "mocked_secret_value"

    # Call the function with test data
    result = get_csv_file_names("US", "category")

    # Assertions
    mock_logger.info.assert_called_with("Bucket_name is your_bucket")
    mock_logger.info.assert_called_with("Namespace:your_namespace")
    mock_logger.info.assert_called_with("file path in gcs Bucket:ltvo_your_namespace/na/US/category")
    mock_logger.info.assert_called_with('final_files info:[{"csv_file_name": "file1.csv"}, {"csv_file_name": "file2.csv"}]')

    expected_result = [{"csv_file_name": "file1.csv"}, {"csv_file_name": "file2.csv"}]
    self.assertEqual(result, expected_result)

---------------------------------------------------


import unittest
from unittest.mock import MagicMock, patch, Mock
from app import app
from services.cost_file_service import get_csv_file_names
#from api.experiment_resource import ExperimentResource, ExperimentsResource, RefreshExperimentsResource
import pytest 


class MockStorageClient:
    def __init__(self, mock_blobs):
        self.mock_blobs = mock_blobs

    class MockBlob:
        def __init__(self, name):
            self.name = name

    def bucket(self, bucket_name):
        return MockStorageClient.MockBucket(self.mock_blobs)

    class MockBucket:
        def __init__(self, mock_blobs):
            self.mock_blobs = mock_blobs

        def list_blobs(self, prefix=None):
            return [MockStorageClient.MockBlob(name) for name in self.mock_blobs]
        

@patch("services.cost_file_service.storage.Client", MockStorageClient)
def test_get_csv_file_names_mocked_function(self, mock_secret_manager, mock_blob, mock_bucket, mock_logger, mock_get_app_configs):
    # Mocking necessary objects
    mock_blobs = ["file1.csv", "file2.csv"]

    mock_get_app_configs.return_value = {
        "ESM_GCS_BUCKET": "your_bucket",
        "NAMESPACE": "your_namespace"
    }

    # Mocking the SecretManagerServiceClient
    mock_secret_manager_instance = mock_secret_manager.return_value
    mock_secret_manager_instance.access_secret_version.return_value.payload.data.decode.return_value = "mocked_secret_value"

    # Call the function with test data
    result = get_csv_file_names("US", "category")

    # Assertions
    mock_logger.info.assert_called_with("Bucket_name is your_bucket")
    mock_logger.info.assert_called_with("Namespace:your_namespace")
    mock_logger.info.assert_called_with("file path in gcs Bucket:ltvo_your_namespace/na/US/category")
    mock_logger.info.assert_called_with('final_files info:[{"csv_file_name": "file1.csv"}, {"csv_file_name": "file2.csv"}]')

    expected_result = [{"csv_file_name": "file1.csv"}, {"csv_file_name": "file2.csv"}]
    self.assertEqual(result, expected_result)






from io import BytesIO
import tempfile
import json
import http

from flask import send_file, jsonify, request
from google.cloud import storage
import pandas as pd
import openpyxl
import csv

from configs import get_app_configs
from utils.app_constants import ESM_GCS_BUCKET, ESM_GCS_BUCKET_CA,NAMESPACE
from utils.filename_generator import generate_namespaced_esm_filename_us
from utils.filename_generator import generate_namespaced_esm_filename_ca
from annotations.param_checks import enable_country_check, enable_country_category_check
import my_logger
app_logger = my_logger.configure_logger()
BUCKET_NAME = 'ltvo-dswb-prod-c423-ltvo-us-t4p8zaot'
generate_fn = {
    'USA': (ESM_GCS_BUCKET, generate_namespaced_esm_filename_us),
    'CAN': (ESM_GCS_BUCKET_CA, generate_namespaced_esm_filename_ca)
}

def download_template_as_xlsx(blob):
    blob_content = blob.download_as_bytes()
    df = pd.read_csv(BytesIO(blob_content))
    df.drop(df.columns[df.columns.str.contains('unnamed', case=False)], 
            axis=1, inplace=True)
    # download template
    with tempfile.NamedTemporaryFile(suffix=".xlsx") as temp:
        df.to_excel(temp, index=False)
        return send_file(
            temp.name, 
            as_attachment=True,  
            mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
        )
    

def download_template(country, template):
    # check if correct country was provided
    if country not in generate_fn: 
        return {
            'message': f"Country {country} not supported."
        }, http.HTTPStatus.NOT_FOUND
    
    storage_client = storage.Client()
    bucket_name = BUCKET_NAME # param from config
    blob_name = 'ESM_STG/' + country + '/' + template + '.csv' 
    
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.get_blob(blob_name)
    if not blob: return

    return download_template_as_xlsx(blob)


def clone_template(country, optimization_id):
    # check if correct country was provided
    if country not in generate_fn: 
        return {
            'message': f"Country {country} not supported."
        }, http.HTTPStatus.NOT_FOUND
    
    storage_client = storage.Client()
    bucket_name = get_app_configs().get(generate_fn[country][0])
    blob_name = generate_fn[country][1](optimization_id)

    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.get_blob(blob_name)
    if not blob: return

    return download_template_as_xlsx(blob)    

    
def upload_template(file, country, optimization_id):

    # check if correct country was provided
    if country not in generate_fn: 
        return {
            'message': f"Country {country} not supported."
        }, http.HTTPStatus.NOT_FOUND

    # prepare client, bucket and filename
    client = storage.Client()
    bucket = client.get_bucket(get_app_configs().get(generate_fn[country][0])) 
    filename = generate_fn[country][1](optimization_id)  

    # convert to csv
    csv_data = None
    try:
        if file.filename.endswith('.xlsx'):
            df = pd.read_excel(file, engine='openpyxl')
            csv_data = df.to_csv(index=False)
        elif file.filename.endswith('.csv'):
            df = pd.read_csv(file)
            csv_data = df.to_csv(index=False)
        else:
            return {
                "message": "Invalid file format. Send xlsx or csv."
            }, http.HTTPStatus.BAD_REQUEST
    except Exception as e:
        return {
            "message": f"Could not read the file, error: {str(e)}"
        }, http.HTTPStatus.UNPROCESSABLE_ENTITY

    # save to a bucket
    bucket = client.get_bucket(get_app_configs().get(generate_fn[country][0]))        
    bucket.blob(filename).upload_from_string(csv_data, content_type='text/csv')

    return {
        "message": filename
    }, http.HTTPStatus.OK

# def create_anonymous_client(*args, **kwargs):
#     creds = AnonymousCredentials()
#     if _NOW() > creds.expiry:
#         creds.refresh(Request())
#     return storage.Client(credentials=creds, _http=grpc.Request())
def get_csv_file_names(country,category):
    storage_client=storage.Client()
    bucket_name=get_app_configs().get(ESM_GCS_BUCKET)
    app_logger.info(f"Bucket_name is {bucket_name}")
    ns=get_app_configs().get(NAMESPACE)
    app_logger.info(f"Namespace:{ns}")
    blob_prefix=f"ltvo_{ns}/na/{country}/{category}"
    app_logger.info(f'file path in gcs Bucket:{blob_prefix}')
    bucket = storage_client.bucket(bucket_name)
    blobs=list(bucket.list_blobs(prefix=blob_prefix))
    csv_file_names=[blob.name for blob in blobs if blob.name.lower().endswith(".csv")]
    final_csv_files=[{"csv_file_name":csv_file}for csv_file in csv_file_names]
    app_logger.info(f'final_files info:{final_csv_files}')
    return final_csv_files

@enable_country_check
@enable_country_category_check
def read_csv_file_content(country,category,file_name):
    storage_client=storage.Client()
    bucket_name=get_app_configs().get(ESM_GCS_BUCKET)
    app_logger.info(f"Bucket_name is {bucket_name}")
    ns=get_app_configs().get(NAMESPACE)
    app_logger.info(f"Namespace:{ns}")
    blob_prefix=f"ltvo_{ns}/na/{country}/{category}"
    app_logger.info(f"file path in GCS Bucket :{blob_prefix}")
    bucket = storage_client.bucket(bucket_name)
    blobs=list(bucket.list_blobs(prefix=blob_prefix))
    csv_file_names=[blob.name for blob in blobs if blob.name.lower().endswith(".csv")]
    try:
        if file_name in csv_file_names:
            blob_name=f"ltvo_{ns}/na/{country}/{category}"+"/"+file_name
            app_logger.info(f"file path for specified file:{blob_name}")
            blob = bucket.blob(blob_name)
            rows = blob.download_as_bytes().decode().splitlines()
            rows_list=list(csv.reader(rows))
            app_logger.info(f"list of rows in csv file:{list(csv.reader(rows))}")
            app_logger.info(f"List of rows in the csv file:{rows_list[0]}")
            header = rows[0].split(',')
            app_logger.info(f"Headers in the csv file:{header}")
            data_list = []
            for row in rows[1:]:
                row_values = row.split(',')
                data_dict = {header[i]: row_values[i] for i in range(len(header))}
                data_list.append(data_dict)
                app_logger.info(f"final_data{data_list}")
                #Convert the list to JSON
                json_data = json.dumps(data_list, indent=2)
                app_logger.info(f"Final Json Data is :{json_data}")
                return json_data
        else:
                return f"Filename{file_name} doesnot exist in {bucket_name}"
    except Exception as e:
        return f"Exception is {str(e)}"

    
