UI will pass a path such as “/na/usa/fhc”

Syntax for the path is /na/{country}/{category} This syntax will match the syntax of where the files are located in the COST bucket

API will return the list of files located in the GCS cost bucket with the above path.

Path in GCS should be namespaced by environment. For example, DEV should have a prefix of /dev/na/usa/fhc

Pass (Category and country) and retrieve the list of tempelate

Array of string

JSON format

Below is the confluence link

version: '3'
name: ltvo-backend

services:
  psql-db:
    image: postgres:13
    container_name: psql-db
    environment:
      POSTGRES_USER: ltvo_ui_dev
      POSTGRES_PASSWORD: password123
    hostname: psql-db
    ports:
      - "5432:5432"
    volumes:
      - local_pgdata:/var/lib/postgresql/data
  gcp_storage:
    image: oittaa/gcp-storage-emulator
    container_name: gcp_storage
    ports:
      - "9023:9023"
    environment:
      PORT: 9023
      HOST: gcp_storage
    volumes:
      - gcp-data:/storage
  pgadmin:
    image: dpage/pgadmin4
    container_name: pgadmin4_container
    restart: always
    ports:
      - "8888:80"
    environment:
      PGADMIN_DEFAULT_EMAIL: test@pg.com
      PGADMIN_DEFAULT_PASSWORD: pass123
    volumes:
      - pgadmin-data:/var/lib/pgadmin
  api:
    container_name: ltvo_api
    build:
      context: ..
      dockerfile: setup/Dockerfile
    image: ltvo-backend
    environment:
      - ACTIVE_PROFILE=LOCAL
      - PG_HOST=psql-db:5432
      - PG_USER=ltvo_ui_dev
      - PG_PASSWORD=password123
      - FLASK_ENV=development
      - FLASK_APP=app
      - STORAGE_EMULATOR_HOST=http://gcp_storage:9023
    entrypoint: /ltvo-backend/setup/entrypoint.sh
    ports:
      - "9090:9090"
    volumes:
      - ..:/ltvo-backend
    depends_on:
      - psql-db

volumes:
  local_pgdata:
  pgadmin-data:
  gcp-data:

GET COST FILE
UI will pass a path such as “/na/usa/fhc/FHC_UNCONSTRAINED”

syntax is /na/{country}/{category}/{template}

API must validate the user has access to the country / category for which they are calling the endpoint. Can use legacy API functionality for this validation.

API must return the json list of deals that are in the csv file at the above path


gcp_storage:
    image: oittaa/gcp-storage-emulator
    container_name: gcp_storage
    ports:
      - "9023:9023"
    environment:
      PORT: 9023
      HOST: gcp_storage
    volumes:
      - gcp-data:/storage



 source_blob = storage_client.list_blobs(bucket, prefix=prefix, delimiter="/")
        copied_list = []
        for blob in source_blob:
            if blob.name.endswith(".csv"):
                source_bucket.copy_blob(blob, destination_bucket, dest_path + blob.name.split("/")[-1])
                copied_list.append({"file_copied" : bucket + "/"+ dest_path + blob.name.split("/")[-1]})
        return copied_list

BUCKET_NAME = 'ltvo-dswb-prod-c423-ltvo-us-t4p8zaot'
if country not in generate_fn: 
        return {
            'message': f"Country {country} not supported."
        }, http.HTTPStatus.NOT_FOUND
    
    storage_client = storage.Client()
    bucket_name = BUCKET_NAME # param from config
    blob_name = 'ESM_STG/' + country + '/' + template + '.csv' 
    
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.get_blob(blob_name)
    if not blob: return



import os
from google.cloud import storage
from flask import Flask, request, jsonify

app = Flask(__name__)

def get_file_names(country, template):
    # Get the bucket name from the environment variable
    bucket_name = os.environ.get('GCS_BUCKET_NAME')

    if not bucket_name:
        return {"error": "GCS_BUCKET_NAME environment variable not set."}

    storage_client = storage.Client()

    # Construct the blob prefix
    blob_prefix = f'ESM_STG/{country}/{template}/'

    # Get the bucket
    bucket = storage_client.bucket(bucket_name)

    # List blobs in the bucket with the specified prefix
    blobs = list(bucket.list_blobs(prefix=blob_prefix))

    file_names = [blob.name.split("/")[-1] for blob in blobs]

    return file_names

@app.route('/get_file_names', methods=['GET'])
def get_file_names_endpoint():
    try:
        # Get parameters from the request
        country = request.args.get('country')
        template = request.args.get('template')

        # Check if required parameters are provided
        if not country or not template:
            return jsonify({"error": "Missing required parameters."}), 400

        # Get file names from the GCS path
        file_names = get_file_names(country, template)

        return jsonify({"file_names": file_names})

    except Exception as e:
        return jsonify({"error": str(e)}), 500

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)


 metadata server.
ltvo_api            | INFO:werkzeug:172.22.0.1 - - [03/Jan/2024 17:25:01] "GET /api/v2/cost_file_names?country=USA&category=FHC HTTP/1.1" 500 -
ltvo_api            | WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 1 of 3. Reason: [Errno 111] Connection refused
ltvo_api            | WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 2 of 3. Reason: [Errno 111] Connection refused
ltvo_api            | WARNING:google.auth.compute_engine._metadata:Compute Engine Metadata server unavailable on attempt 3 of 3. Reason: [Errno 111] Connection refused
ltvo_api            | WARNING:google.auth._default:Authentication failed using Compute Engine authentication due to unavailable metadata server.

api.add_resource(CostFileResource, '/api/v2/cost_file')
api.add_resource(CostFileResource, '/api/v2/cost_file_names')
api.add_resource(OptimizationRunListResource, '/api/v2/optimization_runs')
api.add_resource(DevOptimizationsResource, '/api/v2/dev_optimization/list')
api.add_resource(OptimizationRunResource, *optimization_run_routes)



from google.cloud._helpers import _NOW

def create_anonymous_client(*args, **kwargs):
    creds = AnonymousCredentials()
    if _NOW() > creds.expiry:
        creds.refresh(Request())
    return storage.Client(credentials=creds, _http=grpc.Request())
'


country=input("Enter country name")
category=input("Enter category name")
from google.cloud import storage
storage_client=storage.Client()

bucket_name="dbce-ltvo-ui-dev-16d3-ltvo-ui-cf-2q88ktf9"
print(bucket_name)

blob_prefix=f"/{country}/{category}/"

bucket = storage_client.bucket(bucket_name)

blobs=list(bucket.list_blobs(prefix=blob_prefix))

file_names=[blob.name.split("/")[-1] for blob in blobs]

print(file_names)


Traceback (most recent call last):
  File "/home/subash_vo/sample.py", line 12, in <module>
    blobs=list(bucket.list_blobs(prefix=blob_prefix))
  File "/usr/local/lib/python3.9/dist-packages/google/api_core/page_iterator.py", line 208, in _items_iter
    for page in self._page_iter(increment=False):
  File "/usr/local/lib/python3.9/dist-packages/google/api_core/page_iterator.py", line 244, in _page_iter
    page = self._next_page()
  File "/usr/local/lib/python3.9/dist-packages/google/api_core/page_iterator.py", line 373, in _next_page
    response = self._get_next_page_response()
  File "/usr/local/lib/python3.9/dist-packages/google/api_core/page_iterator.py", line 432, in _get_next_page_response
    return self.api_request(
  File "/usr/local/lib/python3.9/dist-packages/google/cloud/storage/_http.py", line 72, in api_request
    return call()
  File "/usr/local/lib/python3.9/dist-packages/google/api_core/retry.py", line 372, in retry_wrapped_func
    return retry_target(
  File "/usr/local/lib/python3.9/dist-packages/google/api_core/retry.py", line 207, in retry_target
    result = target()
  File "/usr/local/lib/python3.9/dist-packages/google/cloud/_http/__init__.py", line 482, in api_request
    response = self._make_request(
  File "/usr/local/lib/python3.9/dist-packages/google/cloud/_http/__init__.py", line 341, in _make_request
    return self._do_request(
  File "/usr/local/lib/python3.9/dist-packages/google/cloud/_http/__init__.py", line 379, in _do_request
    return self.http.request(
  File "/usr/local/lib/python3.9/dist-packages/google/auth/transport/requests.py", line 537, in request
    self.credentials.before_request(auth_request, method, url, request_headers)
  File "/usr/local/lib/python3.9/dist-packages/google/auth/credentials.py", line 175, in before_request
    self.refresh(request)
  File "/usr/local/lib/python3.9/dist-packages/google/auth/compute_engine/credentials.py", line 116, in refresh
    self._retrieve_info(request)
  File "/usr/local/lib/python3.9/dist-packages/google/auth/compute_engine/credentials.py", line 93, in _retrieve_info
    self._service_account_email = info["email"]
TypeError: string indices must be integers



[['name', 'age', 'gender'], ['subash', '23', 'male']]



country=input("Enter country name")
category=input("Enter category name")
from google.cloud import storage
storage_client=storage.Client()

bucket_name="dbce-ltvo-ui-dev-16d3-ltvo-ui-cf-2q88ktf9"
print(bucket_name)
blob_prefix=f"ltvo_dev/na/{country}/{category}"
#print("dbce-ltvo-ui-dev-16d3-ltvo-ui-cf-2q88ktf9/ltvo_dev/na/usa/fhc")
bucket = storage_client.bucket(bucket_name)

blobs=list(bucket.list_blobs(prefix=blob_prefix))
file_names=[blob.name.split("/")[-1] for blob in blobs]
print(file_names)


Traceback (most recent call last):
  File "/usr/local/lib/python3.9/dist-packages/google/cloud/storage/blob.py", line 4332, in _prep_and_do_download
    self._do_download(
  File "/usr/local/lib/python3.9/dist-packages/google/cloud/storage/blob.py", line 987, in _do_download
    response = download.consume(transport, timeout=timeout)
  File "/usr/local/lib/python3.9/dist-packages/google/resumable_media/requests/download.py", line 237, in consume
    return _request_helpers.wait_and_retry(
  File "/usr/local/lib/python3.9/dist-packages/google/resumable_media/requests/_request_helpers.py", line 155, in wait_and_retry
    response = func()
  File "/usr/local/lib/python3.9/dist-packages/google/resumable_media/requests/download.py", line 219, in retriable_request
    self._process_response(result)
  File "/usr/local/lib/python3.9/dist-packages/google/resumable_media/_download.py", line 188, in _process_response
    _helpers.require_status_code(
  File "/usr/local/lib/python3.9/dist-packages/google/resumable_media/_helpers.py", line 108, in require_status_code




for csv_file in csv_file_names:
    print(f"\nProcessing CSV file:{csv_file}")
    print(csv_file)
    file = bucket.blob(csv_file)
    print(file.exists())
    
    try:
        # blob=bucket.blob(csv_file)
        # content=blob.download_as_text()
        # print(content)
        blob_name=csv_file
        blob = bucket.blob(blob_name)
        rows = blob.download_as_bytes().decode().splitlines()
        rows_list=list(csv.reader(rows))
        print(list(csv.reader(rows)))
        print(rows_list[0])
    raise common.InvalidResponse(
google.resumable_media.common.InvalidResponse: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/subash_vo/sample.py", line 21, in <module>
    content=blob.download_as_text()
  File "/usr/local/lib/python3.9/dist-packages/google/cloud/storage/blob.py", line 1630, in download_as_text
    data = self.download_as_bytes(
  File "/usr/local/lib/python3.9/dist-packages/google/cloud/storage/blob.py", line 1405, in download_as_bytes
    self._prep_and_do_download(
  File "/usr/local/lib/python3.9/dist-packages/google/cloud/storage/blob.py", line 4345, in _prep_and_do_download
    _raise_from_invalid_response(exc)
  File "/usr/local/lib/python3.9/dist-packages/google/cloud/storage/blob.py", line 4791, in _raise_from_invalid_response
    raise exceptions.from_http_status(response.status_code, message, response=response)
google.api_core.exceptions.NotFound: 404 GET https://storage.googleapis.com/download/storage/v1/b/dbce-ltvo-ui-dev-16d3-ltvo-ui-cf-2q88ktf9/o/sample1.csv?alt=media: No such object: dbce-ltvo-ui-dev-16d3-ltvo-ui-cf-2q88ktf9/sample1.csv: ('Request failed with status code', 404, 'Expected one of', <HTTPStatus.OK: 200>, <HTTPStatus.PARTIAL_CONTENT: 206>)
subash_vo@cloudshell:~$ 



 header = rows[0].split(',')
        
        # Create a list of dictionaries
        data_list = []
        for row in rows[1:]:
            row_values = row.split(',')
            data_dict = {header[i]: row_values[i] for i in range(len(header))}
            data_list.append(data_dict)
        
        # Print the list of dictionaries
        print(data_list)
        
        # Convert the list to JSON
        json_data = json.dumps(data_list, indent=2)
        print(json_data)




def get_csv_file_names(country,category):
    storage_client=storage.Client()
    bucket_name=get_app_configs().get(ESM_GCS_BUCKET)
    ns=get_app_configs().get(NAMESPACE)
    blob_prefix=f"ltvo_{ns}/na/{country}/{category}"
    bucket = storage_client.bucket(bucket_name)
    blobs=list(bucket.list_blobs(prefix=blob_prefix))
    csv_file_names=[blob.name for blob in blobs if blob.name.lower().endswith(".csv")]
    return csv_file_names

@enable_country_check
@enable_country_category_check
def read_csv_file_content(country,category):
    storage_client=storage.Client()
    bucket_name=get_app_configs().get(ESM_GCS_BUCKET)
    ns=get_app_configs().get(NAMESPACE)
    blob_prefix=f"ltvo_{ns}/na/{country}/{category}"
    bucket = storage_client.bucket(bucket_name)
    blobs=list(bucket.list_blobs(prefix=blob_prefix))
    csv_file_names=[blob.name for blob in blobs if blob.name.lower().endswith(".csv")]
    for csv_file in csv_file_names:
        print(f"\nProcessing CSV file:{csv_file}")
        file = bucket.blob(csv_file)
        if file.exists():
            print("File Exists")
        else:
            print("File Doesnot exists")
        try:
            blob_name=csv_file
            blob = bucket.blob(blob_name)
            rows = blob.download_as_bytes().decode().splitlines()
            rows_list=list(csv.reader(rows))
            print(list(csv.reader(rows)))
            print(rows_list[0])
            header = rows[0].split(',')
            data_list = []
            for row in rows[1:]:
                row_values = row.split(',')
                data_dict = {header[i]: row_values[i] for i in range(len(header))}
                data_list.append({"template":data_dict})
            print(data_list)
            json_data = json.dumps(data_list, indent=2)
            return json_data
        except Exception as e:
            print(f"Exception is {str(e)}")




class CostFileNameResource(Resource):
    def get(self):
        try:
            args=request.args 
            country=args.get('country')
            category=args.get('category')
            # if not country or not category:
            #     return {"error":"Missing Required parameters"}, 400
            # bucket_name="test_bucket"
            # if not bucket_name:
            #     return {"error":"bucket_name is not set"}, 500
            file_names=get_csv_file_names(country,category)
            return {"file_names":file_names}
        except Exception as e:
            return {"error":str(e)}, 500

class CostFileContentResource(Resource):
    def get(self):
        try:
            args=request.args 
            country=args.get('country')
            category=args.get('category')
            # if not country or not category:
            #     return {"error":"Missing Required parameters"}, 400
            # bucket_name="test_bucket"
            # if not bucket_name:
            #     return {"error":"bucket_name is not set"}, 500
            file_content=read_csv_file_content(country,category)
            return file_content
        except Exception as e:
            return {"error":str(e)}, 500


api.add_resource(CostFileResource, '/api/v2/cost_file')
api.add_resource(CostFileNameResource, '/api/v2/cost_file_names')




import unittest
from unittest.mock import patch, MagicMock
from your_module import get_csv_file_names, read_csv_file_content, CostFileNameResource, CostFileContentResource

class TestYourModule(unittest.TestCase):

    @patch('your_module.storage.Client')
    @patch('your_module.get_app_configs')
    def test_get_csv_file_names(self, mock_get_app_configs, mock_storage_client):
        mock_get_app_configs.return_value = {'ESM_GCS_BUCKET': 'your_bucket', 'NAMESPACE': 'your_namespace'}
        mock_bucket = MagicMock()
        mock_storage_client.return_value.bucket.return_value = mock_bucket
        mock_blob1 = MagicMock(name='blob1', name='path/to/your_file1.csv')
        mock_blob2 = MagicMock(name='blob2', name='path/to/your_file2.csv')
        mock_bucket.list_blobs.return_value = [mock_blob1, mock_blob2]

        result = get_csv_file_names('your_country', 'your_category')

        self.assertEqual(result, ['path/to/your_file1.csv', 'path/to/your_file2.csv'])

    @patch('your_module.storage.Client')
    @patch('your_module.get_app_configs')
    def test_read_csv_file_content(self, mock_get_app_configs, mock_storage_client):
        mock_get_app_configs.return_value = {'ESM_GCS_BUCKET': 'your_bucket', 'NAMESPACE': 'your_namespace'}
        mock_bucket = MagicMock()
        mock_storage_client.return_value.bucket.return_value = mock_bucket
        mock_blob = MagicMock()
        mock_blob.download_as_bytes.return_value.decode.return_value = 'header1,header2\nvalue1,value2'
        mock_bucket.blob.return_value = mock_blob









from google.cloud import storage
import json
import pandas as pd
storage_client=storage.Client()
import csv
from get_appconstants import dev_configs
def get_csv_file_content(country,category,filename):
    storage_client=storage.Client()
    bucket_name=dev_configs.get('bucket_name')
    ns=dev_configs.get('ns')
    print(f"Bucket_name is {bucket_name}")
    print(f"Namespace is {ns}")
    blob_prefix=f"ltvo_{ns}/na/{country}/{category}"
    print(f"File path is {blob_prefix}")
    bucket = storage_client.bucket(bucket_name)
    blobs=list(bucket.list_blobs(prefix=blob_prefix))
    csv_file_names=[blob.name.split("/")[-1] for blob in blobs if blob.name.lower().endswith(".csv")]
    final_csv_files=[{"csv_file_name":csv_file}for csv_file in csv_file_names]
    return final_csv_files
country=input("Enter Country name")
category = input("Enter Category name")
file_name=input("Enter Csv_file name")
csv_file_names=get_csv_file_names(country,category,file_name)
print(csv_file_names)

        result = read_csv_file_content('your_country', 'your_category')

        expected_result = '[{"template": {"header1": "value1", "header2": "value2"}}]'
        self.assertEqual(result, expected_result)

    def test_cost_file_name_resource(self):
        resource = CostFileNameResource()
        with patch.object(resource, 'get', return_value={'file_names': ['file1.csv', 'file2.csv']}):
            response = resource.get()
        self.assertEqual(response, {'file_names': ['file1.csv', 'file2.csv']})

    def test_cost_file_content_resource(self):
        resource = CostFileContentResource()
        with patch.object(resource, 'get', return_value='{"template": {"header1": "value1", "header2": "value2"}}'):
            response = resource.get()
        self.assertEqual(response, '{"template": {"header1": "value1", "header2": "value2"}}')

if __name__ == '__main__':
    unittest.main()
